\chapter{Propuesta de solución}\label{chapter:proposal}

La implementacion  de este trabajo esta dividida en dos partes fundamentales: Extraccion del AST y trabajo con machine learning.

- Extraccion del AST: En esta parte se utiliza la herramienta ANTLR de C\# para crear el ast de los proyectos. Luego de este ast se extraen los features para su posterior uso.

- Trabajo con machine learning: Se estudian diferentes técnicas para determinar la semejanza entre 2 proyectos. El uso de modelos de deteccion de oulayers para agrupar los vectores de similitud de los proyectos que son distintos entre si.


\section{Extraccion del AST}

ANTLR (Another Tool for Language Recognition) es una poderosa herramienta que se utiliza para generar analizadores léxicos y sintácticos a partir de gramáticas definidas por el usuario. Es ampliamente utilizada en la compilación y el procesamiento de lenguajes, ya que permite transformar el código fuente en estructuras de datos que pueden ser fácilmente manipuladas. En este caso, ANTLR se emplea para extraer Árboles de Sintaxis Abstracta (AST) a partir del código fuente de programas escritos en C\#. Fue necesario hacer ligeras modificaciones sobre la gramática, pues ANTLR tenia una gramatica desactualizada. \\

Una vez que se ha generado el AST, se puede manipular y analizar utilizando las estructuras de datos proporcionadas por ANTLR. Por ejemplo, se pueden recorrer los nodos del árbol, extraer información específica o transformar el AST para diferentes propósitos, en este caso se utilizó el listener proporcionado por ANTLR para recorrer el árbol y extraer los featurues. \\

\section{Extraccion de features}

En el análisis de similitud de código y detección de patrones, es crucial extraer características relevantes que capturen la estructura y el comportamiento del código. Para este propósito, se implementó una clase denominada {\bf FeatureExtractorListener}, que extiende la funcionalidad de ANTLR para analizar el código fuente en C\#. A continuación, se presenta una descripción detallada del proceso de extracción de características y la importancia de cada característica extraída. \\

\begin{enumerate}
	\item Estructura del AST:
    		\begin{itemize}
			\item {\bf total\_nodes:} Número total de nodos en el AST.
			\item {\bf max\_depth:} Profundidad máxima del AST.
		\end{itemize}
		
	 \item Declaraciones y Variables:
    \begin{itemize}
        \item {\bf variables:} Número de variables locales.
        \item {\bf constants:} Número de constantes declaradas.
        \item {\bf variable\_names:} Conjunto de nombres de variables y sus tipos.
        \item {\bf number\_of\_tuples:} Número de variables de tipo tupla.
        \item {\bf lists:} Número de listas declaradas.
        \item {\bf dicts:} Número de diccionarios declarados.
    \end{itemize}
    Las variables y las constantes son fundamentales para entender el estado y el flujo de datos en el código. La variedad y el tipo de estructuras de datos utilizadas (tuplas, listas, diccionarios) también proporcionan información sobre el estilo de programación y la complejidad del código.

    \item Declaraciones de Métodos y Clases:
    \begin{itemize}
        \item {\bf methods:} Número de métodos declarados.
        \item {\bf method\_names:} Conjunto de nombres de métodos.
        \item {\bf method\_return\_types:} Conjunto de tipos de retorno de métodos.
        \item {\bf method\_parameters:} Lista de parámetros de métodos.
        \item {\bf classes:} Número de clases declaradas.
        \item {\bf class\_names:} Conjunto de nombres de clases.
        \item {\bf abstract\_classes:} Número de clases abstractas.
        \item {\bf sealed\_classes:} Número de clases selladas.
        \item {\bf interfaces:} Número de interfaces declaradas.
        \item {\bf interface\_names:} Conjunto de nombres de interfaces.
    \end{itemize}
    La estructura y los nombres de los métodos y clases proporcionan información sobre la organización y modularidad del código. Los métodos y sus parámetros son esenciales para entender la funcionalidad del código, mientras que las clases y sus tipos (abstractas, selladas) indican la arquitectura de la aplicación.

    \item Estructuras de Control:
    \begin{itemize}
        \item {\bf control\_structures\_if:} Número de sentencias if.
        \item {\bf control\_structures\_switch:} Número de sentencias switch.
        \item {\bf control\_structures\_for:} Número de bucles for.
        \item {\bf control\_structures\_while:} Número de bucles while.
        \item {\bf control\_structures\_dowhile:} Número de bucles do-while.
        \item {\bf try\_catch\_blocks:} Número de bloques try-catch.
    \end{itemize}
    Las estructuras de control son fundamentales para comprender el flujo del programa y su lógica. Un mayor número de estructuras de control indica una lógica más compleja y ramificada.

    \item Modificadores y Accesibilidad:
    \begin{itemize}
        \item {\bf access\_modifiers\_public:} Número de elementos públicos.
        \item {\bf access\_modifiers\_private:} Número de elementos privados.
        \item {\bf access\_modifiers\_protected:} Número de elementos protegidos.
        \item {\bf access\_modifiers\_internal:} Número de elementos internos.
        \item {\bf access\_modifiers\_static:} Número de elementos estáticos.
        \item {\bf access\_modifiers\_protected\_internal:} Número de elementos protegidos internos.
        \item {\bf access\_modifiers\_private\_protected:} Número de elementos privados protegidos.
    \end{itemize}
    Los modificadores de acceso proporcionan información sobre la encapsulación y visibilidad de los componentes del código. La prevalencia de ciertos modificadores puede indicar prácticas de diseño y seguridad en el código.

    \item Modificadores Específicos:
    \begin{itemize}
        \item {\bf modifier\_readonly:} Número de elementos readonly.
        \item {\bf modifier\_volatile:} Número de elementos volatile.
        \item {\bf modifier\_virtual:} Número de elementos virtual.
        \item {\bf modifier\_override:} Número de elementos override.
        \item {\bf modifier\_new:} Número de elementos new.
        \item {\bf modifier\_partial:} Número de elementos partial.
        \item {\bf modifier\_extern:} Número de elementos extern.
        \item {\bf modifier\_unsafe:} Número de elementos unsafe.
        \item {\bf modifier\_async:} Número de elementos async.
    \end{itemize}
    Estos modificadores específicos indican características avanzadas y patrones de diseño en el código, como la concurrencia (async), la seguridad (unsafe) y la herencia (override, virtual).

    \item Llamadas a Librerías y LINQ:
    \begin{itemize}
        \item {\bf library\_call\_console:} Número de llamadas a la librería Console.
        \item {\bf library\_call\_math:} Número de llamadas a la librería Math.
        \item {\bf linq\_queries\_select:} Número de consultas LINQ Select.
        \item {\bf linq\_queries\_where:} Número de consultas LINQ Where.
        \item {\bf linq\_queries\_orderBy:} Número de consultas LINQ OrderBy.
        \item {\bf linq\_queries\_groupBy:} Número de consultas LINQ GroupBy.
        \item {\bf linq\_queries\_join:} Número de consultas LINQ Join.
        \item {\bf linq\_queries\_sum:} Número de consultas LINQ Sum.
        \item {\bf linq\_queries\_count:} Número de consultas LINQ Count.
    \end{itemize}
    Las llamadas a librerías y consultas LINQ proporcionan información sobre el uso de funcionalidades estándar y el manejo de colecciones de datos en el código.

    \item Otras Características:
    \begin{itemize}
        \item {\bf number\_of\_lambdas:} Número de expresiones lambda.
        \item {\bf number\_of\_getters:} Número de métodos get.
        \item {\bf number\_of\_setters:} Número de métodos set.
        \item {\bf number\_of\_namespaces:} Número de espacios de nombres.
        \item {\bf enums:} Número de enumeraciones.
        \item {\bf enum\_names:} Conjunto de nombres de enumeraciones.
        \item {\bf delegates:} Número de delegados.
        \item {\bf delegate\_names:} Conjunto de nombres de delegados.
        \item {\bf node\_count:} Conteo de nodos por tipo.
    \end{itemize}
    Estas características adicionales proporcionan una visión más completa de las capacidades del código, su organización y las prácticas de programación utilizadas.

 
\end{enumerate}
    
        
La extracción de características con FeatureExtractorListener permite capturar una amplia gama de aspectos del código fuente en C\#, desde su estructura y complejidad hasta los patrones de diseño y las prácticas de programación. Esta información es crucial para tareas como la detección de similitudes de código, la evaluación de la calidad del código y la identificación de posibles plagios. La implementación y el análisis detallado de estas características proporcionan una base sólida para mejorar la precisión y efectividad de las herramientas de análisis de código.

\section{Preparacion del dataset}

Para preparar el dataset utilizado en el análisis de similitud de código, se requirió convertir los nombres de variables, métodos y otros identificadores en vectores de características numéricas. Este proceso se llevó a cabo utilizando la técnica de embeddings con Word2Vec. A continuación, se explica qué es Word2Vec, cómo funciona y por qué se eligió para esta tarea \cite{mikolov2013efficient}. \\

\subsubsection{Uso de Word2Vec en la Preparación del Dataset}
Word2Vec es una técnica de aprendizaje profundo que transforma palabras en vectores de números de alta dimensión, conocidos como embeddings. Los modelos Word2Vec se entrenan utilizando grandes corpus de texto y capturan relaciones semánticas entre las palabras.  \\

En el contexto del análisis de similitud de código, los nombres de variables, métodos y otros identificadores en el código fuente juegan un papel crucial. Estos nombres pueden proporcionar información semántica valiosa sobre la funcionalidad y el propósito de diferentes partes del código. Por ejemplo, los identificadores que se usan de manera similar en diferentes contextos tendrán embeddings similares. Sin embargo, los identificadores en el código no están estructurados de manera que las máquinas puedan comprender fácilmente sus relaciones semánticas. Para abordar este desafío, se utilizó Word2Vec para convertir estos identificadores en embeddings. El proceso involucró los siguientes pasos:

\begin{enumerate}
	\item Extracción de Identificadores: Se extrajeron todos los nombres de variables, métodos, clases, interfaces, enumeraciones y delegados del código fuente utilizando la clase FeatureExtractorListener.
	
	\item Entrenamiento de Word2Vec: Se utilizó un corpus de identificadores extraídos de múltiples proyectos de C\# para entrenar el modelo Word2Vec. El modelo aprendió las relaciones semánticas entre los diferentes identificadores en el contexto del código.
	
	\item Conversión a Embeddings: Cada identificador extraído se convirtió en un vector de características numéricas utilizando el modelo Word2Vec entrenado. Luego se halla el promedio entre todos los vectores por feature correspondiente para asegurar que todos las caracteristicas de vectores tengan la misma dimensión. Estos vectores capturan la semántica y el contexto de los identificadores en el código.
	 
\end{enumerate}

\subsubsection{Nuevo dataset}
Para maximizar la cantidad de datos disponibles y reflejar de manera efectiva la similitud entre proyectos, se creó un nuevo dataset que contiene todos los pares posibles (2 a 2) de proyectos del conjunto de datos original. En este proceso, para cada par de proyectos, se calcula y almacena la diferencia entre sus vectores correspondientes.

Este enfoque tiene varias ventajas significativas:

\begin{itemize}
	\item {\bf Incremento en la Cantidad de Datos:} Generar todos los pares posibles de proyectos incrementa exponencialmente el número de instancias en el dataset, proporcionando una base de datos más rica y diversa para entrenar modelos de aprendizaje automático.
	
	\item {\bf Etiquetado Automático de Datos:} Una ventaja adicional de calcular la distancia entre los datos es que permite disponer de algunos datos etiquetados. Aunque no se puede afirmar si un proyecto individual es original o una copia, al calcular las distancias dos a dos, se puede asegurar que un par de proyectos provenientes de diferentes tipos de proyectos no son copias uno del otro. Esto proporciona etiquetas adicionales que mejoran la calidad del entrenamiento del modelo.

	\item {\bf Captura de Relaciones Detalladas:} Al almacenar la diferencia entre los vectores de cada par de proyectos, se capturan las distancias y relaciones específicas entre todos los proyectos. Esto permite que el modelo de machine learning pueda aprender las sutilezas de las similitudes y diferencias entre distintos proyectos.
	
	\item {\bf Mejora en la Precisión del Modelo:} Con un mayor volumen de datos y la inclusión de las distancias entre pares, se espera que el modelo tenga un mejor desempeño en la tarea de detección de similitudes. La precisión del modelo se ve beneficiada al disponer de más ejemplos que reflejan una amplia gama de variaciones y similitudes.
	
	\item {\bf Refinamiento de las Métricas de Similitud:} Este método permite que se utilicen métricas de similitud más refinadas y precisas, ya que cada par de proyectos se compara de manera detallada. 

\end{itemize}

En resumen, la creación de este nuevo dataset con todos los pares posibles y sus diferencias vectoriales no solo aumenta la cantidad de datos disponibles, sino que también enriquece la información sobre las relaciones entre proyectos, mejorando así la capacidad del modelo para detectar similitudes de manera precisa y eficiente.  

\section{Probando diferentes modelos}

\subsubsection{SNN}
La primera solución abordada fue implementar una red neuronal siamesa para comparar dos proyectos. Estas redes funcionan con dos entradas, procesando cada una a través de la misma arquitectura de red para obtener representaciones vectoriales comparables. La red neuronal siamesa mide la distancia entre estos vectores para determinar la similitud entre los proyectos.\\

Para esta solución, se separaron todos los posibles pares de proyectos y se les asignó una etiqueta: 0 si los proyectos no son copias (ambos originales) y 1 si los proyectos son copias. Los datos se convirtieron en vectores de características y el conjunto de datos se constituyó con los pares de vectores correspondientes y sus etiquetas.\\

Sin embargo, surgió el problema de la generación de copias de proyectos para etiquetar correctamente los datos. Esto llevó a la necesidad de buscar otra solución más efectiva a la carencia de datos etiquetados.


\subsubsection{Clustering}

El clustering es un proceso de aprendizaje no supervisado, utilizado para agrupar datos sin conocer de antemano la etiqueta o categoría de cada dato. En lugar de depender de etiquetas predefinidas, los algoritmos de clustering analizan la proximidad y similitud entre los datos para formar grupos o clústeres basados en características compartidas. Cada clúster agrupa datos que son similares entre sí, pero que difieren de los datos en otros clústeres. Este enfoque es particularmente útil en situaciones donde se quiere descubrir patrones, estructuras o relaciones inherentes sin la necesidad de etiquetas predefinidas.

Se aplicaron distintos enfoques de clustering, entre ellos:
\begin{itemize}
    \item \textbf{K-Means}: Agrupa datos en \(k\) clústeres basándose en la distancia euclidiana a los centroides, que se ajustan iterativamente. Aquí se hizo uso del método del codo para determinar el parámetro \(k\) a utilizar.
    \item \textbf{DBSCAN (Density-Based Spatial Clustering of Applications with Noise)}: Identifica clústeres basados en la densidad de puntos, permitiendo descubrir clústeres de forma arbitraria y detectar ruido.
    \item \textbf{Agglomerative Clustering}: Un enfoque jerárquico que une iterativamente los puntos o clústeres más cercanos, formando un dendrograma.
    \item \textbf{MeanShift}: Agrupa datos desplazando iterativamente los puntos hacia la densidad máxima de puntos más cercana, identificando modos en la densidad de datos.
\end{itemize}

Aunque algunos de estos algoritmos mostraron buenas métricas en ciertos aspectos, no lograron resolver completamente el problema planteado. 

\section{Modelos utilizados}

\subsubsection{One-Class SVM}

One-Class Support Vector Machine (One-Class SVM) es una variante del algoritmo SVM (Support Vector Machine) diseñada para la detección de anomalías y para identificar datos que difieren del conjunto de datos de entrenamiento. One-Class SVM es particularmente útil en escenarios donde solo se dispone de datos de una clase (la clase "normal") y se desea detectar cualquier dato que sea diferente o anómalo.\\

One-Class SVM se basa en la idea de encontrar una función que sea positiva en la región donde se encuentran la mayoría de los datos de entrenamiento y negativa en otras regiones. Este enfoque ayuda a identificar las regiones del espacio de características donde se encuentran los datos \"normales\" y marcar como anómalos los datos que caen fuera de estas regiones.\\

\textbf{Funcionamiento:}

\begin{enumerate}
    \item \textbf{Transformación del Espacio de Características:}
    \begin{itemize}
        \item Se utiliza una función de kernel para mapear los datos de entrada a un espacio de características de mayor dimensión.
        \item Los kernels comunes incluyen el kernel lineal, polinómico, radial (RBF) y sigmoide.
    \end{itemize}

    \item \textbf{Maximización del Margen:}
    \begin{itemize}
        \item One-Class SVM trata de encontrar un hiperplano en el espacio de características transformado que maximice la distancia de los puntos de datos a este hiperplano.
        \item La mayoría de los datos de entrenamiento deben estar en un lado del hiperplano (considerados normales) y los puntos fuera de este margen son considerados anomalías.
    \end{itemize}
\end{enumerate}

\subsection*{Fórmula Matemática}

El problema de optimización para One-Class SVM se define como:

\[ \min_{\mathbf{w}, \rho, \xi} \frac{1}{2} \|\mathbf{w}\|^2 + \frac{1}{\nu n} \sum_{i=1}^{n} \xi_i - \rho \]

sujeto a:

\[ (\mathbf{w} \cdot \phi(\mathbf{x}_i)) \geq \rho - \xi_i, \quad \xi_i \geq 0, \quad i = 1, \ldots, n \]

Donde:
\begin{itemize}
    \item \( \mathbf{w} \) es el vector de pesos.
    \item \( \phi(\mathbf{x}_i) \) es la transformación del espacio de características mediante el kernel.
    \item \( \rho \) es el umbral que separa los datos normales de las anomalías.
    \item \( \xi_i \) son las variables de holgura que permiten que algunos puntos estén en el lado equivocado del margen.
    \item \( \nu \) es un parámetro que controla el trade-off entre la fracción de puntos de datos que son permitidos estar en el lado equivocado del margen y la regularización del modelo.
\end{itemize}


\textbf{Ventajas:}
\begin{itemize}
    \item Efectivo en escenarios donde solo se dispone de datos de una clase.
    \item Flexible con el uso de diferentes kernels para capturar la estructura de datos no lineales.
\end{itemize}


\subsubsection{Isolation forest}

Isolation Forest es un algoritmo de aprendizaje automático no supervisado diseñado para detectar anomalías en datos. Fue introducido por Fei Tony Liu, Kai Ming Ting y Zhi-Hua Zhou en 2008 \cite{liu2008isolation}. A diferencia de muchos algoritmos de detección de anomalías que se basan en la densidad o la distancia, Isolation Forest se basa en el concepto de \"aislamiento\".\\

La idea central de Isolation Forest es que las anomalías son puntos de datos que son "fáciles de aislar". Dado que los valores anómalos son raros y diferentes del resto de los datos, deben requerir menos divisiones para ser aislados. \\

\textbf{Funcionamiento: }

\begin{enumerate}
    \item \textbf{Construcción de Árboles de Aislamiento:}
    \begin{itemize}
        \item Se construyen múltiples árboles de aislamiento (iTrees). Cada árbol se construye seleccionando aleatoriamente una característica y luego eligiendo aleatoriamente un valor de división entre los valores mínimo y máximo de esa característica.
        \item Este proceso se repite hasta que cada punto de datos esté aislado en una hoja del árbol o hasta que se alcance una profundidad máxima predefinida.
    \end{itemize}

    \item \textbf{Cálculo de la Longitud del Camino:}
    \begin{itemize}
        \item La longitud del camino de un punto de datos es el número de divisiones necesarias para aislar ese punto.
        \item Las anomalías, siendo más fáciles de aislar, tienden a tener caminos más cortos.
    \end{itemize}

    \item \textbf{Puntuación de Anomalía:}
    \begin{itemize}
        \item La puntuación de anomalía se basa en la longitud del camino.
        \item Si un punto tiene una puntuación de anomalía alta, es considerado una anomalía.
    \end{itemize}
\end{enumerate}

\subsection*{Fórmula Matemática}

Para calcular la puntuación de anomalía, se utiliza la siguiente fórmula:

\[
s(x, n) = 2^{-\frac{E(h(x))}{c(n)}}
\]

Donde:
\begin{itemize}
    \item \( s(x, n) \) es la puntuación de anomalía del punto \( x \) en un conjunto de datos de tamaño \( n \).
    \item \( E(h(x)) \) es la longitud media del camino de aislamiento para el punto \( x \) calculada sobre múltiples árboles.
    \item \( c(n) \) es una constante de normalización calculada como:
\end{itemize}

\[
c(n) = 2H(n-1) - \left( \frac{2(n-1)}{n} \right)
\]

Aquí, \( H(i) \) es la \( i \)-ésima armónica:

\[
H(i) = \sum_{k=1}^{i} \frac{1}{k}
\]

\textbf{Ventajas:}
\begin{itemize}
    \item Eficiente y escalable a grandes conjuntos de datos.
    \item No requiere un modelo previo de datos normales o anómalos.
    \item Puede manejar datos de alta dimensionalidad.
\end{itemize}


\subsubsection{Por que elegir estos modelos?}

Ambos métodos, Isolation Forest y One-Class SVM, fueron entrenados con pares de proyectos que son diferentes entre sí. Esto significa que aprendieron las diferencias inherentes entre proyectos que podrían contener anomalías o fraudes. Al entrenar con proyectos que representan variabilidad y diversidad en términos de estructuras de código, estilos de programación y lógica implementada, los modelos pueden capturar mejor las características que distinguen un proyecto legítimo de uno potencialmente fraudulento.

\subsection*{Descripción de la Región de Similitud}

El objetivo de detectar fraude entre proyectos de C\# de primer año implica identificar anomalías en la similitud estructural y lógica entre dos proyectos. Isolation Forest y One-Class SVM son adecuados para este propósito porque:

\begin{itemize}
    \item \textbf{Isolation Forest:} Al modelar la estructura de los datos mediante la creación de múltiples árboles de decisión, Isolation Forest identifica anomalías como puntos que son fáciles de aislar en el espacio de características. Esto es efectivo para detectar proyectos que se desvían significativamente de la norma en términos de estructura y contenido.
    
    \item \textbf{One-Class SVM:} Al encontrar una frontera que encapsula la mayoría de los datos normales en el espacio de características, One-Class SVM puede detectar puntos de datos que están muy lejos de esta frontera. Esto es útil para identificar proyectos que muestran características inusuales o no esperadas en comparación con los proyectos normales.
\end{itemize}

El dominio específico de proyectos de C\# de primer año facilita el uso de estos algoritmos. Dado que los proyectos son de un nivel académico inicial, es probable que existan variaciones predecibles pero distintivas entre los proyectos legítimos y aquellos que podrían contener fraude o anomalías. Esto hace que describir la región de los vectores de similitud entre proyectos distintos sea más accesible y factible, ya que las diferencias entre proyectos normales y anómalos pueden ser más discernibles y significativas en este contexto específico.


