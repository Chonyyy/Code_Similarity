\chapter{Propuesta de solución}\label{chapter:proposal}

La implementacion  de este trabajo esta dividida en tres partes fundamentales: Extraccion del AST, trabajo con los datos y explicación del modelo.

- Extraccion del AST: En esta parte se utiliza la herramienta ANTLR de C\# para crear el ast de los proyectos. Luego de este ast se extraen los features para su posterior uso.

- Trabajo con los datos: Los conjuntos de features de cada proyecto se convierten en vectores de características.

- Modelo utilizado: Se implementa el modelo de Red Neuronal Siamesa para determinar la semejanza entre 2 proyectos.

\chapter{Extracción de features del AST}\label{chapter:proposal}
\section{Extraccion del AST}

 Se emplea ANTLR para extraer Árboles de Sintaxis Abstracta (AST) a partir del código fuente de programas escritos en C\#. Convierte una gramática de lenguaje en código que puede generar un árbol de sintaxis. Fue necesario hacer ligeras modificaciones sobre la gramática, pues ANTLR cuenta con la gramatica 8.0 de C\#. \\
 A continuación, se explica su funcionamiento y las etapas principales en su flujo de trabajo:
 
 \begin{enumerate}
    \item \textbf{Definición de la Gramática:}
    Primero, se define la gramática del lenguaje en un archivo \texttt{.g4}. Este archivo incluye:
    \begin{itemize}
        \item \textbf{Reglas léxicas (tokens):} especifican los elementos más básicos, como palabras clave, identificadores, operadores, números, etc.
        \item \textbf{Reglas sintácticas:} describen cómo se combinan los tokens para formar sentencias válidas en el lenguaje.
    \end{itemize}
    Cada regla léxica o sintáctica tiene un nombre y una expresión que define qué secuencias de caracteres o estructuras pueden corresponderse con esa regla.

    \item \textbf{Generación del Lexer y el Parser:}
    ANTLR toma el archivo de gramática \texttt{.g4} y genera clases en un lenguaje de programación (como Java, Python, C\#, etc.) que implementan el lexer (analizador léxico) y el parser (analizador sintáctico).
    \begin{itemize}
        \item \textbf{Lexer:} identifica tokens en el texto de entrada. Por ejemplo, en una expresión matemática, reconoce números, operadores, paréntesis, etc.
        \item \textbf{Parser:} usa estos tokens para construir una estructura jerárquica que representa la gramática definida, lo cual permite reconocer la estructura completa del código o texto de entrada.
    \end{itemize}

    \item \textbf{Análisis del Código o Texto de Entrada:} Con el lexer y parser generados, se puede analizar el código fuente. Este proceso produce un árbol de sintaxis que representa la estructura jerárquica del código según la gramática.

    \item \textbf{Creación del Árbol de Sintaxis Abstracta (AST):} ANTLR facilita la creación de un AST, una representación simplificada que retiene la estructura lógica del código, omitiendo detalles innecesarios para ciertos tipos de análisis.

    \item \textbf{Recorridos y Transformaciones en el AST:} Una vez construido el AST, es posible recorrerlo para realizar análisis adicionales, transformaciones o para interpretar el código. ANTLR proporciona métodos para recorrer este árbol y manipular los nodos según las reglas definidas en la gramática, en este caso se utilizó el listener proporcionado por ANTLR para recorrer el árbol y extraer los features.
\end{enumerate}



\section{Extraccion de features}

En el análisis de similitud de código y detección de patrones, es necesario extraer características relevantes que capturen la estructura y el comportamiento del código. Estas características, conocidas como {\bf features}, representan los aspectos más importantes de los datos, permitiendo analizar y comparar fragmentos de código de manera efectiva. Para este propósito, se implementó una clase denominada {\bf FeatureExtractorListener}, que extiende la funcionalidad de ANTLR para analizar el código fuente en C\#. A continuación, se presenta una descripción detallada del proceso de extracción de características y la importancia de cada una. \\

\begin{enumerate}
	\item Estructura del AST:
    		\begin{itemize}
			\item {\bf total\_nodes:} Número total de nodos en el AST.
			\item {\bf max\_depth:} Profundidad máxima del AST.
		\end{itemize}
		
	 \item Declaraciones y Variables:
    \begin{itemize}
        \item {\bf variables:} Número de variables locales.
        \item {\bf constants:} Número de constantes declaradas.
        \item {\bf variable\_names:} Conjunto de nombres de variables y sus tipos.
        \item {\bf number\_of\_tuples:} Número de variables de tipo tupla.
        \item {\bf lists:} Número de listas declaradas.
        \item {\bf dicts:} Número de diccionarios declarados.
    \end{itemize}
    
Las variables y constantes son elementos clave en cualquier programa, ya que almacenan y mantienen valores que pueden cambiar o permanecer fijos durante la ejecución. Comprender el uso de estas entidades en el código permite analizar cómo se manipulan los datos, identificar el flujo de información y observar cómo evolucionan los valores a lo largo del programa. Las variables revelan el comportamiento dinámico del código, mientras que las constantes indican valores fijos que definen parámetros o condiciones estables dentro del flujo de ejecución.\\

Además, la variedad y el tipo de estructuras de datos empleadas, como tuplas, listas y diccionarios, aportan información importante sobre el enfoque y la complejidad del código. Por ejemplo, el uso de estructuras de datos más avanzadas, como diccionarios anidados o listas de objetos, puede reflejar una mayor abstracción y modularidad en el diseño, mientras que estructuras más simples pueden indicar un código directo y menos complejo. Estas elecciones también proporcionan información sobre el estilo de programación del autor y sus preferencias en cuanto a la organización y manipulación de datos.

    \item Declaraciones de Métodos y Clases:
    \begin{itemize}
        \item {\bf methods:} Número de métodos declarados.
        \item {\bf method\_names:} Conjunto de nombres de métodos.
        \item {\bf method\_return\_types:} Conjunto de tipos de retorno de métodos.
        \item {\bf method\_parameters:} Lista de parámetros de métodos.
        \item {\bf classes:} Número de clases declaradas.
        \item {\bf class\_names:} Conjunto de nombres de clases.
        \item {\bf abstract\_classes:} Número de clases abstractas.
        \item {\bf sealed\_classes:} Número de clases selladas.
        \item {\bf interfaces:} Número de interfaces declaradas.
        \item {\bf interface\_names:} Conjunto de nombres de interfaces.
    \end{itemize}
    
    La estructura y los nombres de los métodos y clases ofrecen información clave sobre la organización, modularidad y legibilidad del código. Los nombres de métodos y clases, cuando están bien definidos y siguen convenciones de nomenclatura clara, actúan como una especie de documentación implícita, ayudando a comprender la función y el propósito de cada componente sin necesidad de examinar cada detalle interno. 

Los métodos y sus parámetros son esenciales para entender la funcionalidad del código. Los métodos representan acciones específicas y los parámetros definen los datos con los que esas acciones trabajan. Al analizar los métodos y los tipos de parámetros que aceptan, se puede deducir cómo las distintas partes del código interactúan y colaboran para realizar tareas. La estructura de los métodos, su nivel de abstracción, y la forma en que interactúan con otros componentes del código revelan la profundidad de la modularidad y el diseño de la aplicación, lo que facilita el análisis de patrones y la identificación de similitudes entre diferentes fragmentos de código.

Por otro lado, las clases y sus tipos (como clases abstractas o selladas) indican la arquitectura y el paradigma de diseño de la aplicación. Las clases abstractas, por ejemplo, representan conceptos generales que definen una estructura básica sin implementación completa, alentando la reutilización y la extensibilidad en el diseño del sistema. Las clases selladas (sealed) limitan la herencia, sugiriendo un diseño más controlado y dirigido a la especificidad. Estas elecciones de diseño reflejan la intención del desarrollador en cuanto a la extensibilidad, la reutilización y la encapsulación, todos ellos principios fundamentales en la programación orientada a objetos.

    \item Estructuras de Control:
    \begin{itemize}
        \item {\bf control\_structures\_if:} Número de sentencias if.
        \item {\bf control\_structures\_switch:} Número de sentencias switch.
        \item {\bf control\_structures\_for:} Número de bucles for.
        \item {\bf control\_structures\_while:} Número de bucles while.
        \item {\bf control\_structures\_dowhile:} Número de bucles do-while.
        \item {\bf try\_catch\_blocks:} Número de bloques try-catch.
    \end{itemize}
    
    Las estructuras de control son fundamentales para comprender el flujo del programa y su lógica. Un mayor número de estructuras de control indica una lógica más compleja y ramificada.

    \item Modificadores y Accesibilidad:
    \begin{itemize}
        \item {\bf access\_modifiers\_public:} Número de elementos públicos.
        \item {\bf access\_modifiers\_private:} Número de elementos privados.
        \item {\bf access\_modifiers\_protected:} Número de elementos protegidos.
        \item {\bf access\_modifiers\_internal:} Número de elementos internos.
        \item {\bf access\_modifiers\_static:} Número de elementos estáticos.
        \item {\bf access\_modifiers\_protected\_internal:} Número de elementos protegidos internos.
        \item {\bf access\_modifiers\_private\_protected:} Número de elementos privados protegidos.
    \end{itemize}
    Los modificadores de acceso proporcionan información sobre la encapsulación y visibilidad de los componentes del código. La prevalencia de ciertos modificadores puede indicar prácticas de diseño y seguridad en el código.

    \item Modificadores Específicos:
    \begin{itemize}
        \item {\bf modifier\_readonly:} Número de elementos readonly.
        \item {\bf modifier\_volatile:} Número de elementos volatile.
        \item {\bf modifier\_virtual:} Número de elementos virtual.
        \item {\bf modifier\_override:} Número de elementos override.
        \item {\bf modifier\_new:} Número de elementos new.
        \item {\bf modifier\_partial:} Número de elementos partial.
        \item {\bf modifier\_extern:} Número de elementos extern.
        \item {\bf modifier\_unsafe:} Número de elementos unsafe.
        \item {\bf modifier\_async:} Número de elementos async.
    \end{itemize}
    
    Estos modificadores específicos reflejan patrones de diseño, control y comportamiento que van más allá de la simple estructura superficial del código. Al analizar modificadores como readonly, volatile o async, se capturan detalles sobre cómo el código maneja la concurrencia, la inmutabilidad y la asincronía, se utilizan para identificar similitudes en la lógica y el flujo de ejecución. Modificadores como virtual y override indican una arquitectura orientada a objetos, esto permite comparar el grado de extensibilidad y personalización en diferentes fragmentos de código. Además, la presencia de unsafe y extern sugiere que el código interactúa con recursos de bajo nivel o externos, lo que proporciona información sobre la complejidad y las dependencias de cada implementación.\\

    \item Llamadas a Librerías y LINQ(Language Integrated Query):
    \begin{itemize}
        \item {\bf library\_call\_console:} Número de llamadas a la librería Console.
        \item {\bf library\_call\_math:} Número de llamadas a la librería Math.
        \item {\bf linq\_queries\_select:} Número de consultas LINQ Select.
        \item {\bf linq\_queries\_where:} Número de consultas LINQ Where.
        \item {\bf linq\_queries\_orderBy:} Número de consultas LINQ OrderBy.
        \item {\bf linq\_queries\_groupBy:} Número de consultas LINQ GroupBy.
        \item {\bf linq\_queries\_join:} Número de consultas LINQ Join.
        \item {\bf linq\_queries\_sum:} Número de consultas LINQ Sum.
        \item {\bf linq\_queries\_count:} Número de consultas LINQ Count.
    \end{itemize}
    
    Las llamadas a librerías y las consultas LINQ ofrecen información sobre cómo el código aprovecha las funcionalidades estándar y gestiona la manipulación de datos. Al utilizar llamadas a librerías, el código accede a un conjunto de herramientas predefinidas y optimizadas. Esto permite deducir la experiencia y el estilo del programador en términos de modularidad y adaptabilidad, aspectos clave en la estructura y lógica del código.\\

Por otro lado, el uso de consultas LINQ para manipular y consultar colecciones de datos refleja un enfoque específico en la optimización y claridad de la manipulación de datos en .NET. LINQ permite un acceso eficiente a estructuras de datos complejas, proporcionando una sintaxis uniforme para realizar operaciones como filtrado, proyección, agrupación y ordenación. La presencia de consultas LINQ en el código puede indicar la preferencia del desarrollador por una sintaxis declarativa y una gestión avanzada de colecciones, que resulta fundamental para identificar similitudes en la forma en que diferentes fragmentos de código manejan datos.

    \item Otras Características:
    \begin{itemize}
        \item {\bf number\_of\_lambdas:} Número de expresiones lambda.
        \item {\bf number\_of\_getters:} Número de métodos get.
        \item {\bf number\_of\_setters:} Número de métodos set.
        \item {\bf number\_of\_namespaces:} Número de espacios de nombres.
        \item {\bf enums:} Número de enumeraciones.
        \item {\bf enum\_names:} Conjunto de nombres de enumeraciones.
        \item {\bf delegates:} Número de delegados.
        \item {\bf delegate\_names:} Conjunto de nombres de delegados.
        \item {\bf node\_count:} Conteo de nodos por tipo.
    \end{itemize}
    Estas características adicionales permiten analizar el código desde distintas perspectivas que revelan aspectos de diseño y organización. Por ejemplo, el número de expresiones lambda indica la frecuencia de uso de funciones anónimas, lo cual puede reflejar una orientación hacia la programación funcional. La cantidad de métodos de acceso ofrece una idea del manejo de encapsulamiento y control de atributos en las clases. La presencia de espacios de nombres sugiere la estructura modular del código y la separación de responsabilidades, lo que facilita la organización y evita conflictos de nombres. Las enumeraciones y los delegados muestran la variedad de estructuras y tipos personalizados utilizados. Finalmente, el conteo de nodos por tipo permite una visión detallada de los elementos específicos del árbol de sintaxis abstracta, lo cual es útil para evaluar la complejidad y el tipo de construcciones empleadas.
 
\end{enumerate}
    
        
La extracción de características con FeatureExtractorListener permite capturar aspectos relevantes del código fuente en C\#, desde su estructura y complejidad hasta los patrones de diseño y las prácticas de programación. La implementación y el análisis detallado de estas características proporcionan una base sólida para mejorar la precisión de las herramientas de análisis de código.



\chapter{Preparacion del dataset}\label{chapter:proposal}

Para preparar el dataset, se convierten los nombres de variables, métodos y otros identificadores de tipo string en vectores de características numéricas, lo cual permite que un modelo de machine learning procese el código de manera efectiva. Este proceso de transformación se realiza utilizando embeddings, una técnica de procesamiento del lenguaje natural, mediante el modelo Word2Vec \cite{mikolov2013efficient}. Estos embeddings se combinan luego con otras características numéricas extraídas del código, como el número de métodos o la cantidad de expresiones lambda, lo cual forma un vector de características completo. Este vector integrado proporciona una descripción multidimensional del código, capturando tanto la estructura como la semántica en una única representación, lo que mejora la capacidad del modelo para detectar patrones y realizar comparaciones entre diferentes fragmentos de código.

\section{Construcción del dataset}
bla bla bla

\section{Word2Vec}

En el contexto del análisis de similitud de código, los nombres de variables, métodos y otros identificadores en el código fuente proporcionan información semántica sobre la funcionalidad y el propósito de diferentes partes del código. Por ejemplo, los identificadores que se usan de manera similar en diferentes contextos tendrán embeddings\footnote{Los embeddings son una técnica de procesamiento de lenguaje natural que convierte el lenguaje humano en vectores matemáticos. Estos vectores son una representación del significado subyacente de las palabras, lo que permite que las computadoras procesen el lenguaje de manera más efectiva.} similares. Sin embargo, los identificadores en el código no están estructurados de manera que las máquinas puedan comprender fácilmente sus relaciones semánticas, para esto se utilizó Word2Vec. El proceso involucró los siguientes pasos:

\begin{enumerate}
	\item Extracción de Identificadores: Se extrajeron todos los nombres de variables, métodos, clases, interfaces, enumeraciones y delegados del código fuente utilizando la clase FeatureExtractorListener.
	
	\item Entrenamiento de Word2Vec: Se utilizó un corpus de identificadores extraídos de múltiples proyectos de C\# para entrenar el modelo Word2Vec. El modelo aprendió las relaciones semánticas entre los diferentes identificadores en el contexto del código.
	
	\item Conversión a Embeddings: Cada identificador extraído se convirtió en un vector de características numéricas utilizando el modelo Word2Vec entrenado. Luego se halla el promedio entre todos los vectores por feature correspondiente para asegurar que todos las caracteristicas de vectores tengan la misma dimensión. Estos vectores capturan la semántica y el contexto de los identificadores en el código.
	 
\end{enumerate}

\section{Dataset de diferencias}

Para maximizar la cantidad de datos disponibles y reflejar de manera efectiva la similitud entre proyectos, se creó un dataset que contiene todos los pares posibles (2 a 2) de proyectos del conjunto de datos original. En este proceso, para cada par de proyectos, se calcula y almacena la diferencia entre sus vectores correspondientes.\\

Este enfoque tiene varias ventajas significativas:

\begin{itemize}
	\item {\bf Incremento en la Cantidad de Datos:} Generar todos los pares posibles de proyectos incrementa exponencialmente el número de instancias en el dataset, proporcionando una base de datos más rica y diversa para entrenar modelos de aprendizaje automático.
	
	\item {\bf Etiquetado Automático de Datos:} Una ventaja de calcular la distancia entre los datos es que permite disponer de algunos datos etiquetados. Aunque no se puede afirmar si un proyecto individual es original o una copia de otro proyecto, al calcular las distancias dos a dos, se puede asegurar que un par de proyectos provenientes de diferentes tipos de proyectos no son copias uno del otro. Esto proporciona etiquetas adicionales que mejoran la calidad del entrenamiento del modelo.

	\item {\bf Captura de Relaciones Detalladas:} Al almacenar la diferencia entre los vectores de cada par de proyectos, se capturan las distancias y relaciones específicas entre todos los proyectos. Esto permite que el modelo de machine learning pueda aprender las sutilezas de las similitudes y diferencias entre distintos proyectos.
	
	\item {\bf Mejora en la Precisión del Modelo:} Con un mayor volumen de datos y la inclusión de las distancias entre pares, se espera que el modelo tenga un mejor desempeño en la tarea de detección de similitudes. La precisión del modelo se ve beneficiada al disponer de más ejemplos que reflejan una amplia gama de variaciones y similitudes.
	
	\item {\bf Refinamiento de las Métricas de Similitud:} Este método permite que se utilicen métricas de similitud precisas, ya que cada par de proyectos se compara de manera detallada. 

\end{itemize}

En resumen, la creación de este dataset con todos los pares posibles y sus diferencias vectoriales no solo aumenta la cantidad de datos disponibles, sino que también enriquece la información sobre las relaciones entre proyectos, mejorando así la capacidad del modelo para detectar similitudes de manera precisa y eficiente.  


\chapter{Redes Neuronales Siamesas para la Similitud de Código}\label{chapter:proposal}

Las redes neuronales siamesas han demostrado ser efectivas en el análisis de similitud de código, debido a su estructura que permite aprender relaciones semánticas profundas entre pares de datos de entrada. En este capítulo se describen los componentes fundamentales de esta arquitectura y su proceso de entrenamiento.

\section{Estructura de una Red Neuronal Siamesa}

La red neuronal siamesa consta de dos subredes idénticas que comparten los mismos parámetros y pesos. Cada subred toma como entrada un fragmento de código, que es representado a través de vectores de características derivados de su Árbol de Sintaxis Abstracta (AST). Las salidas de ambas subredes son vectores de alta dimensión que capturan características semánticas y estructurales del código procesado. Posteriormente, estos vectores se comparan mediante una función de distancia, la similitud de coseno, que cuantifica el grado de similitud entre los fragmentos de código.\\

Esta red esta compuesta por tres capas densas.\\

Al emplear una red siamesa, el modelo puede determinar qué tan “cercanos” o “distantes” son dos fragmentos de código en el espacio semántico aprendido, lo cual es crucial en aplicaciones donde se busca identificar equivalencias estructurales o funcionales.

\section{Proceso de Entrenamiento y Función de Pérdida}

El entrenamiento de una red neuronal siamesa para la detección de similitud de código generalmente se lleva a cabo mediante la \textit{pérdida de contraste} (contrastive loss) o la \textit{pérdida de triplete} (triplet loss). Estas funciones de pérdida están diseñadas para reducir la distancia entre los vectores de fragmentos de código que cumplen la misma funcionalidad (pares similares) y aumentar la distancia entre los vectores de fragmentos de código que cumplen funciones diferentes (pares disimilares), en este trabajo se utilizará la tecnica de \textit{pérdida de contraste}. Esta función minimiza la distancia entre representaciones de fragmentos de código similares y maximiza la distancia entre representaciones de fragmentos disímiles. Para cada par de fragmentos de código, esta función penaliza la red de acuerdo con la cercanía o lejanía de las representaciones generadas.


\section{Aplicación de Redes Siamesas en la Similitud de Código en C\#}

Para aplicar redes siamesas en la detección de similitud de código en C\#, cada fragmento de código se transforma en un vector de características utilizando su AST. Este árbol captura la estructura sintáctica y lógica del código, permitiendo que el modelo aprenda relaciones más profundas que las posibles mediante representaciones textuales o léxicas. Los vectores de características pueden incluir información sobre las operaciones, los flujos de control, los nombres de funciones, y otros elementos sintácticos relevantes del código en C\#. \\

La representación vectorial del AST es crítica, ya que permite que la red siamés se enfoque en aspectos estructurales y funcionales del código, ignorando variaciones superficiales, como cambios en el nombre de variables o formato. De esta forma, el modelo se vuelve robusto frente a modificaciones superficiales que no alteran la lógica del programa, pero susceptibles de engañar métodos más básicos de detección de similitud.

\section{Futuras Direcciones}

 Se me ocurre la idea de entrenar el modelo con codigo de java(existe mas dataset con java que C\#) para que vaya teniendo una base de que es una copia de codigo y luego especializarlo con codigos de C\#, porque pienso que esto funcionaria? Los vectores vas a tener la misma estructura tanto para C# como para Java (cantidad de if, de while etc.) . Aqui tendria que investigar las diferencias entre C\# y Java, generar el AST con ANTLR para Java hacer de nuevo la extraccion de caracteristicas si los ast no tienen la misma estructura que es lo mas probable, muy pesado de hacer... y ya despues de esto tendria el vector de caracteristicas para Java y a entrenar el modelo con java, luego investigar como se reutiliza un modelo ya entrenado y especializarlo, esto tiene el nombre en ML como fine tuning. Tambien puede que aparezca magicamente el modelo ya entrenado con Java y sea cargarlo.  Segun ChatGPT:
 
 Este método implica dos fases principales:
Pre-entrenamiento (Pre-training):
En esta fase, el modelo se entrena con un conjunto de datos grande y general. Este entrenamiento inicial permite al modelo aprender características generales y representaciones útiles de los datos.
Fine-tuning (Ajuste fino):
Después del pre-entrenamiento, el modelo se vuelve a entrenar con un conjunto de datos más pequeño y específico para la tarea en cuestión. Durante este proceso, se ajustan los pesos del modelo para adaptarse mejor a la tarea específica.
Características clave del transfer learning:
Eficiencia: Permite aprovechar el conocimiento adquirido en tareas generales para mejorar el rendimiento en tareas específicas.
Menor necesidad de datos: Requiere menos datos para la tarea específica, ya que el modelo ya ha aprendido características generales útiles.
Tiempo de entrenamiento reducido: El fine-tuning suele ser más rápido que entrenar un modelo desde cero.
Mejor rendimiento: A menudo resulta en un mejor rendimiento en la tarea específica, especialmente cuando los datos específicos son limitados.
Versatilidad: Se puede aplicar en diversos campos como visión por computadora, procesamiento de lenguaje natural, y reconocimiento de voz.
En el contexto de tu proyecto de similitud de código, podrías considerar pre-entrenar tu modelo siamés en un conjunto grande de proyectos de código de diversos lenguajes, y luego hacer fine-tuning con un conjunto más pequeño de proyectos C# específicos. Esto podría ayudar a tu modelo a capturar características generales de la estructura del código y luego especializarse en las particularidades del C#.

Despues de esta maravillosa idea que no se si funcione esta seguir probando con chatGPT a ver si me quiere dar codigos plagueados bien, entrenar a algun LLM hacerle prompt ingeeniering para ensennarle a plaguear y que luego nos de el codigo que queremos.

No se me ocurre mas nada para el dataset.

