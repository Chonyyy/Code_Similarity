\chapter{Propuesta de solución}\label{chapter:proposal}

La implementacion  de este trabajo esta dividida en tres partes fundamentales: Extraccion del AST, trabajo con los datos y explicación del modelo.

- Extraccion del AST: En esta parte se utiliza la herramienta ANTLR de C\# para crear el ast de los proyectos. Luego de este ast se extraen los features para su posterior uso.

- Trabajo con los datos: Los conjuntos de features de cada proyecto se convierten en vectores de características.

- Modelo utilizado: Se implementa el modelo de Red Neuronal Siamesa para determinar la semejanza entre 2 proyectos.

\chapter{Extracción de features del AST}\label{chapter:proposal}
\section{Extraccion del AST}

 Se emplea ANTLR para extraer Árboles de Sintaxis Abstracta (AST) a partir del código fuente de programas escritos en C\#. Convierte una gramática de lenguaje en código que puede generar un árbol de sintaxis. Fue necesario hacer ligeras modificaciones sobre la gramática, pues ANTLR cuenta con la gramatica 8.0 de C\#. \\
 A continuación, se explica su funcionamiento y las etapas principales en su flujo de trabajo:
 
 \begin{enumerate}
    \item \textbf{Definición de la Gramática:}
    Primero, se define la gramática del lenguaje en un archivo \texttt{.g4}. Este archivo incluye:
    \begin{itemize}
        \item \textbf{Reglas léxicas (tokens):} especifican los elementos más básicos, como palabras clave, identificadores, operadores, números, etc.
        \item \textbf{Reglas sintácticas:} describen cómo se combinan los tokens para formar sentencias válidas en el lenguaje.
    \end{itemize}
    Cada regla léxica o sintáctica tiene un nombre y una expresión que define qué secuencias de caracteres o estructuras pueden corresponderse con esa regla.

    \item \textbf{Generación del Lexer y el Parser:}
    ANTLR toma el archivo de gramática \texttt{.g4} y genera clases en un lenguaje de programación (como Java, Python, C\#, etc.) que implementan el lexer (analizador léxico) y el parser (analizador sintáctico).
    \begin{itemize}
        \item \textbf{Lexer:} identifica tokens en el texto de entrada. Por ejemplo, en una expresión matemática, reconoce números, operadores, paréntesis, etc.
        \item \textbf{Parser:} usa estos tokens para construir una estructura jerárquica que representa la gramática definida, lo cual permite reconocer la estructura completa del código o texto de entrada.
    \end{itemize}

    \item \textbf{Análisis del Código o Texto de Entrada:} Con el lexer y parser generados, se puede analizar el código fuente. Este proceso produce un árbol de sintaxis que representa la estructura jerárquica del código según la gramática.

    \item \textbf{Creación del Árbol de Sintaxis Abstracta (AST):} ANTLR facilita la creación de un AST, una representación simplificada que retiene la estructura lógica del código, omitiendo detalles innecesarios para ciertos tipos de análisis.

    \item \textbf{Recorridos y Transformaciones en el AST:} Una vez construido el AST, es posible recorrerlo para realizar análisis adicionales, transformaciones o para interpretar el código. ANTLR proporciona métodos para recorrer este árbol y manipular los nodos según las reglas definidas en la gramática, en este caso se utilizó el listener llamado {\bf CSharpParserListener}, proporcionado por ANTLR para recorrer el árbol y extraer los features.
\end{enumerate}



\section{Extraccion de features}

En el análisis de similitud de código y detección de patrones, es necesario extraer características relevantes que capturen la estructura y el comportamiento del código. Estas características, conocidas como {\bf features}, representan los aspectos más importantes de los datos, permitiendo analizar y comparar fragmentos de código de manera efectiva. Para este propósito, se implementó una clase llamada {\bf FeatureExtractorListener}, que extiende la funcionalidad de CSharpParserListener para analizar el código fuente en C\#. A continuación, se presenta una descripción detallada del proceso de extracción de características y la importancia de cada una. \\

\begin{enumerate}
	\item Estructura del AST:
    		\begin{itemize}
			\item {\bf total\_nodes:} Número total de nodos en el AST.
			\item {\bf max\_depth:} Profundidad máxima del AST.
		\end{itemize}
		
	 \item Declaraciones y Variables:
    \begin{itemize}
        \item {\bf variables:} Número de variables locales.
        \item {\bf constants:} Número de constantes declaradas.
        \item {\bf variable\_names:} Conjunto de nombres de variables y sus tipos.
        \item {\bf number\_of\_tuples:} Número de variables de tipo tupla.
        \item {\bf lists:} Número de listas declaradas.
        \item {\bf dicts:} Número de diccionarios declarados.
    \end{itemize}
    
Las variables y constantes son elementos clave en cualquier programa, ya que almacenan y mantienen valores que pueden cambiar o permanecer fijos durante la ejecución. Comprender el uso de estas entidades en el código permite analizar cómo se manipulan los datos, identificar el flujo de información y observar cómo evolucionan los valores a lo largo del programa. Las variables revelan el comportamiento dinámico del código, mientras que las constantes indican valores fijos que definen parámetros o condiciones estables dentro del flujo de ejecución.\\

Además, la variedad y el tipo de estructuras de datos empleadas, como tuplas, listas y diccionarios, aportan información importante sobre el enfoque y la complejidad del código. Por ejemplo, el uso de estructuras de datos más avanzadas, como diccionarios anidados o listas de objetos, puede reflejar una mayor abstracción y modularidad en el diseño, mientras que estructuras más simples pueden indicar un código directo y menos complejo. Estas elecciones también proporcionan información sobre el estilo de programación del autor y sus preferencias en cuanto a la organización y manipulación de datos.

    \item Declaraciones de Métodos y Clases:
    \begin{itemize}
        \item {\bf methods:} Número de métodos declarados.
        \item {\bf method\_names:} Conjunto de nombres de métodos.
        \item {\bf method\_return\_types:} Conjunto de tipos de retorno de métodos.
        \item {\bf method\_parameters:} Lista de parámetros de métodos.
        \item {\bf classes:} Número de clases declaradas.
        \item {\bf class\_names:} Conjunto de nombres de clases.
        \item {\bf abstract\_classes:} Número de clases abstractas.
        \item {\bf sealed\_classes:} Número de clases selladas.
        \item {\bf interfaces:} Número de interfaces declaradas.
        \item {\bf interface\_names:} Conjunto de nombres de interfaces.
    \end{itemize}
    
    La estructura y los nombres de los métodos y clases ofrecen información clave sobre la organización, modularidad y legibilidad del código. Los nombres de métodos y clases, cuando están bien definidos y siguen convenciones de nomenclatura clara, actúan como una especie de documentación implícita, ayudando a comprender la función y el propósito de cada componente sin necesidad de examinar cada detalle interno. 

Los métodos y sus parámetros son esenciales para entender la funcionalidad del código. Los métodos representan acciones específicas y los parámetros definen los datos con los que esas acciones trabajan. Al analizar los métodos y los tipos de parámetros que aceptan, se puede deducir cómo las distintas partes del código interactúan y colaboran para realizar tareas. La estructura de los métodos, su nivel de abstracción, y la forma en que interactúan con otros componentes del código revelan la profundidad de la modularidad y el diseño de la aplicación, lo que facilita el análisis de patrones y la identificación de similitudes entre diferentes fragmentos de código.

Por otro lado, las clases y sus tipos (como clases abstractas o selladas) indican la arquitectura y el paradigma de diseño de la aplicación. Las clases abstractas, por ejemplo, representan conceptos generales que definen una estructura básica sin implementación completa, alentando la reutilización y la extensibilidad en el diseño del sistema. Las clases selladas (sealed) limitan la herencia, sugiriendo un diseño más controlado y dirigido a la especificidad. Estas elecciones de diseño reflejan la intención del desarrollador en cuanto a la extensibilidad, la reutilización y la encapsulación, todos ellos principios fundamentales en la programación orientada a objetos.

    \item Estructuras de Control:
    \begin{itemize}
        \item {\bf control\_structures\_if:} Número de sentencias if.
        \item {\bf control\_structures\_switch:} Número de sentencias switch.
        \item {\bf control\_structures\_for:} Número de bucles for.
        \item {\bf control\_structures\_while:} Número de bucles while.
        \item {\bf control\_structures\_dowhile:} Número de bucles do-while.
        \item {\bf try\_catch\_blocks:} Número de bloques try-catch.
    \end{itemize}
    
    Las estructuras de control son fundamentales para comprender el flujo del programa y su lógica. Un mayor número de estructuras de control indica una lógica más compleja y ramificada.

    \item Modificadores y Accesibilidad:
    \begin{itemize}
        \item {\bf access\_modifiers\_public:} Número de elementos públicos.
        \item {\bf access\_modifiers\_private:} Número de elementos privados.
        \item {\bf access\_modifiers\_protected:} Número de elementos protegidos.
        \item {\bf access\_modifiers\_internal:} Número de elementos internos.
        \item {\bf access\_modifiers\_static:} Número de elementos estáticos.
        \item {\bf access\_modifiers\_protected\_internal:} Número de elementos protegidos internos.
        \item {\bf access\_modifiers\_private\_protected:} Número de elementos privados protegidos.
    \end{itemize}
    Los modificadores de acceso proporcionan información sobre la encapsulación y visibilidad de los componentes del código. La prevalencia de ciertos modificadores puede indicar prácticas de diseño y seguridad en el código.

    \item Modificadores Específicos:
    \begin{itemize}
        \item {\bf modifier\_readonly:} Número de elementos readonly.
        \item {\bf modifier\_volatile:} Número de elementos volatile.
        \item {\bf modifier\_virtual:} Número de elementos virtual.
        \item {\bf modifier\_override:} Número de elementos override.
        \item {\bf modifier\_new:} Número de elementos new.
        \item {\bf modifier\_partial:} Número de elementos partial.
        \item {\bf modifier\_extern:} Número de elementos extern.
        \item {\bf modifier\_unsafe:} Número de elementos unsafe.
        \item {\bf modifier\_async:} Número de elementos async.
    \end{itemize}
    
    Estos modificadores específicos reflejan patrones de diseño, control y comportamiento que van más allá de la simple estructura superficial del código. Al analizar modificadores como readonly, volatile o async, se capturan detalles sobre cómo el código maneja la concurrencia, la inmutabilidad y la asincronía, se utilizan para identificar similitudes en la lógica y el flujo de ejecución. Modificadores como virtual y override indican una arquitectura orientada a objetos, esto permite comparar el grado de extensibilidad y personalización en diferentes fragmentos de código. Además, la presencia de unsafe y extern sugiere que el código interactúa con recursos de bajo nivel o externos, lo que proporciona información sobre la complejidad y las dependencias de cada implementación.\\

    \item Llamadas a Librerías y LINQ(Language Integrated Query):
    \begin{itemize}
        \item {\bf library\_call\_console:} Número de llamadas a la librería Console.
        \item {\bf library\_call\_math:} Número de llamadas a la librería Math.
        \item {\bf linq\_queries\_select:} Número de consultas LINQ Select.
        \item {\bf linq\_queries\_where:} Número de consultas LINQ Where.
        \item {\bf linq\_queries\_orderBy:} Número de consultas LINQ OrderBy.
        \item {\bf linq\_queries\_groupBy:} Número de consultas LINQ GroupBy.
        \item {\bf linq\_queries\_join:} Número de consultas LINQ Join.
        \item {\bf linq\_queries\_sum:} Número de consultas LINQ Sum.
        \item {\bf linq\_queries\_count:} Número de consultas LINQ Count.
    \end{itemize}
    
    Las llamadas a librerías y las consultas LINQ ofrecen información sobre cómo el código aprovecha las funcionalidades estándar y gestiona la manipulación de datos. Al utilizar llamadas a librerías, el código accede a un conjunto de herramientas predefinidas y optimizadas. Esto permite deducir la experiencia y el estilo del programador en términos de modularidad y adaptabilidad, aspectos clave en la estructura y lógica del código.\\

Por otro lado, el uso de consultas LINQ para manipular y consultar colecciones de datos refleja un enfoque específico en la optimización y claridad de la manipulación de datos en .NET. LINQ permite un acceso eficiente a estructuras de datos complejas, proporcionando una sintaxis uniforme para realizar operaciones como filtrado, proyección, agrupación y ordenación. La presencia de consultas LINQ en el código puede indicar la preferencia del desarrollador por una sintaxis declarativa y una gestión avanzada de colecciones, que resulta fundamental para identificar similitudes en la forma en que diferentes fragmentos de código manejan datos.

    \item Otras Características:
    \begin{itemize}
        \item {\bf number\_of\_lambdas:} Número de expresiones lambda.
        \item {\bf number\_of\_getters:} Número de métodos get.
        \item {\bf number\_of\_setters:} Número de métodos set.
        \item {\bf number\_of\_namespaces:} Número de espacios de nombres.
        \item {\bf enums:} Número de enumeraciones.
        \item {\bf enum\_names:} Conjunto de nombres de enumeraciones.
        \item {\bf delegates:} Número de delegados.
        \item {\bf delegate\_names:} Conjunto de nombres de delegados.
        \item {\bf node\_count:} Conteo de nodos por tipo.
    \end{itemize}
    Estas características adicionales permiten analizar el código desde distintas perspectivas que revelan aspectos de diseño y organización. Por ejemplo, el número de expresiones lambda indica la frecuencia de uso de funciones anónimas, lo cual puede reflejar una orientación hacia la programación funcional. La cantidad de métodos de acceso ofrece una idea del manejo de encapsulamiento y control de atributos en las clases. La presencia de espacios de nombres sugiere la estructura modular del código y la separación de responsabilidades, lo que facilita la organización y evita conflictos de nombres. Las enumeraciones y los delegados muestran la variedad de estructuras y tipos personalizados utilizados. Finalmente, el conteo de nodos por tipo permite una visión detallada de los elementos específicos del árbol de sintaxis abstracta, lo cual es útil para evaluar la complejidad y el tipo de construcciones empleadas.
 
\end{enumerate}
    
        
La extracción de características con FeatureExtractorListener permite capturar aspectos relevantes del código fuente en C\#, desde su estructura y complejidad hasta los patrones de diseño y las prácticas de programación. La implementación y el análisis detallado de estas características proporcionan una base sólida para mejorar la precisión de las herramientas de análisis de código.



\chapter{Preparacion del dataset}\label{chapter:proposal}

Para preparar el dataset, se convierten los nombres de variables, métodos y otros identificadores de tipo string en vectores de características numéricas, lo cual permite que un modelo de machine learning procese el código de manera efectiva. Este proceso de transformación se realiza utilizando embeddings, una técnica de procesamiento del lenguaje natural, mediante el modelo Word2Vec \cite{mikolov2013efficient}. Estos embeddings se combinan con otras características numéricas extraídas del código, como el número de métodos o la cantidad de expresiones lambda, lo cual forma un vector de características completo. Este vector integrado proporciona una descripción multidimensional del código, capturando tanto la estructura como la semántica en una única representación, lo que mejora la capacidad del modelo para detectar patrones y realizar comparaciones entre diferentes fragmentos de código.

\section{Construcción del dataset}

El dataset de códigos en C\# está compuesto por pares de archivos que mantienen la misma funcionalidad pero presentan variaciones en la estructura y nomenclatura. Cada par está compuesto por un archivo original y su respectiva copia, con cambios mínimos y modificaciones que buscan simular escenarios típicos en los que el código mantiene su esencia lógica pero experimenta alteraciones en la sintaxis y organización. Cuenta con 116 archivos generados por ChatGPT, donde la mitad son originales y la mitad son copias.

\begin{enumerate}
    \item \textbf{Creación de Código Base}: Se crearon múltiples fragmentos de código en C\# que abarcan una variedad de funciones comunes, como cálculos geométricos, operaciones matemáticas, manipulación de cadenas de texto y validaciones de números. Estos fragmentos de código están diseñados para ejecutar funciones claras y específicas. 

    \item \textbf{Generación de Copias con Modificaciones}: Para cada archivo original, se generó una copia modificada siguiendo varias estrategias de variación:
    \begin{itemize}
        \item \textit{Renombramiento de Clases, Métodos y Variables}: Los nombres de clases, métodos y variables fueron modificados en las copias para simular diferencias en nomenclatura sin cambiar el objetivo funcional del código.
        \item \textit{Modificación de Estructura de Código}: Se alteraron estructuras sintácticas, como el tipo de bucles (de \texttt{for} a \texttt{while}) o la reestructuración de condiciones lógicas. Estas modificaciones permiten que la copia mantenga la misma funcionalidad que el original, pero con una apariencia diferente en términos de código.
        \item \textit{Ajustes en la Organización}: Algunos fragmentos fueron reorganizados en cuanto al orden de ejecución o el uso de estructuras de control alternativas, manteniendo los resultados finales consistentes con el código original.
    \end{itemize}
\end{enumerate}

\section{Dataset de diferencias}

Para maximizar la cantidad de datos disponibles y reflejar de manera efectiva la similitud entre proyectos, se creó un dataset que contiene todos los pares posibles (2 a 2) de proyectos del conjunto de datos original. En este proceso, para cada par de proyectos, se calcula y almacena la diferencia entre sus vectores correspondientes.\\

Este enfoque tiene varias ventajas significativas:

\begin{itemize}
	\item {\bf Incremento en la Cantidad de Datos:} Generar todos los pares posibles de proyectos incrementa exponencialmente el número de instancias en el dataset, proporcionando una base de datos más rica y diversa para entrenar modelos de aprendizaje automático.
	
	\item {\bf Etiquetado Automático de Datos:} Una ventaja de calcular la distancia entre los datos es que permite disponer de algunos datos etiquetados. Aunque no se puede afirmar si un proyecto individual es original o una copia de otro proyecto, al calcular las distancias dos a dos, se puede asegurar que un par de proyectos provenientes de diferentes tipos de proyectos no son copias uno del otro. Esto proporciona etiquetas adicionales que mejoran la calidad del entrenamiento del modelo.

	\item {\bf Captura de Relaciones Detalladas:} Al almacenar la diferencia entre los vectores de cada par de proyectos, se capturan las distancias y relaciones específicas entre todos los proyectos. Esto permite que el modelo de machine learning pueda aprender las sutilezas de las similitudes y diferencias entre distintos proyectos.
	
	\item {\bf Mejora en la Precisión del Modelo:} Con un mayor volumen de datos y la inclusión de las distancias entre pares, se espera que el modelo tenga un mejor desempeño en la tarea de detección de similitudes. La precisión del modelo se ve beneficiada al disponer de más ejemplos que reflejan una amplia gama de variaciones y similitudes.
	
	\item {\bf Refinamiento de las Métricas de Similitud:} Este método permite que se utilicen métricas de similitud precisas, ya que cada par de proyectos se compara de manera detallada. 

\end{itemize}

En resumen, la creación de este dataset con todos los pares posibles y sus diferencias vectoriales no solo aumenta la cantidad de datos disponibles, sino que también enriquece la información sobre las relaciones entre proyectos, mejorando así la capacidad del modelo para detectar similitudes de manera precisa y eficiente.  


\section{Word2Vec}

En el contexto del análisis de similitud de código, los nombres de variables, métodos y otros identificadores en el código fuente proporcionan información semántica sobre la funcionalidad y el propósito de diferentes partes del código. Por ejemplo, los identificadores que se usan de manera similar en diferentes contextos tendrán embeddings\footnote{Los embeddings son una técnica de procesamiento de lenguaje natural que convierte el lenguaje humano en vectores matemáticos. Estos vectores son una representación del significado subyacente de las palabras, lo que permite que las computadoras procesen el lenguaje de manera más efectiva.} similares. Sin embargo, los identificadores en el código no están estructurados de manera que las máquinas puedan comprender fácilmente sus relaciones semánticas, para esto se utilizó Word2Vec. El proceso involucró los siguientes pasos:

\begin{enumerate}
	\item \textit{Extracción de Identificadores}: Se extrajeron todos los nombres de variables, métodos, clases, interfaces, enumeraciones y delegados del código fuente utilizando la clase FeatureExtractorListener.
	
	\item \textit{Entrenamiento de Word2Vec}: Se utilizó un corpus de identificadores extraídos de múltiples proyectos de C\# para entrenar el modelo Word2Vec. El modelo aprendió las relaciones semánticas entre los diferentes identificadores en el contexto del código.
	
	\item \textit{Conversión a Embeddings}: Cada identificador extraído se convirtió en un vector de características numéricas utilizando el modelo Word2Vec entrenado. Luego se halla el promedio entre todos los vectores por feature correspondiente para asegurar que todos las caracteristicas de vectores tengan la misma dimensión. Estos vectores capturan la semántica y el contexto de los identificadores en el código.
	 
\end{enumerate}


\chapter{Redes Neuronales Siamesas para la Similitud de Código}\label{chapter:proposal}

Las redes neuronales siamesas se emplearon para analizar y capturar tanto la estructura como el comportamiento lógico del código. Este enfoque facilita la identificación de patrones y relaciones que trascienden las comparaciones textuales o sintácticas, proporcionando una mayor precisión en los resultados obtenidos. En este capítulo se detallan los componentes esenciales de esta arquitectura y el proceso llevado a cabo para su entrenamiento.


\section{Estructura de la Red Neuronal Siamesa}

La red neuronal siamesa utilizada está compuesta por dos subredes idénticas que comparten parámetros y pesos, una propiedad que garantiza que ambas subredes procesen las entradas de manera uniforme. Cada subred recibe como entrada un vector de características derivado del Árbol de Sintaxis Abstracta (AST) del código fuente, transformado previamente en una representación numérica adecuada para su procesamiento mediante redes neuronales. Este enfoque permite abstraer detalles superficiales del código, como nombres de variables o cambios en el formato, enfocándose en aspectos estructurales y funcionales que capturan la lógica del programa.

La arquitectura base de cada subred incluye capas de convolución unidimensional (Conv1D), las cuales son  utilizadas para capturar patrones locales en secuencias de datos. Estas capas están diseñadas para procesar vectores de entrada de forma eficiente y extraer características significativas mediante la convolución, seguida de operaciones de *pooling* que reducen la dimensionalidad y retienen las características más relevantes. La estructura general de la subred se describe como sigue:

\begin{itemize}
    \item Una capa de \textit{Conv1D} con 16 filtros y un tamaño de kernel de 8, activada mediante ReLU, seguida de una operación de \textit{max pooling} con un tamaño de ventana de 2.
    \item Una segunda capa de \textit{Conv1D} con 32 filtros y un tamaño de kernel de 4, también activada mediante ReLU, seguida de \textit{max pooling}.
    \item Una capa de \textit{Flatten} que transforma los tensores resultantes en un vector de características de alta dimensión.
    \item Tres capas densas, con 512, 256 y 10 neuronas respectivamente, utilizando ReLU para las dos primeras y softmax para la última. Esta última capa proyecta las características en un espacio de 10 dimensiones.
\end{itemize}

La salida de cada subred es un vector de alta dimensión \( \mathbf{h} \in \mathbb{R}^d \), donde \( d \) es el número de neuronas en la capa de salida. Estos vectores encapsulan las propiedades semánticas y estructurales del código procesado.

La comparación entre las dos salidas se realiza mediante una función de distancia. En este caso, se emplea la distancia euclidiana, definida como:

\[
d(\mathbf{h}_a, \mathbf{h}_b) = \sqrt{\sum_{i=1}^{d} (\mathbf{h}_{a,i} - \mathbf{h}_{b,i})^2},
\]

donde \( \mathbf{h}_a \) y \( \mathbf{h}_b \) son las representaciones vectoriales generadas por las dos subredes para los fragmentos de código \( a \) y \( b \). Esta métrica cuantifica el grado de similitud entre los fragmentos, donde distancias menores indican mayor similitud.

\section{Proceso de Entrenamiento y Función de Pérdida}

El modelo fue entrenado utilizando la \textit{pérdida de contraste} (contrastive loss), una función utilizada en redes siamesas para tareas de comparación y similitud. Esta función busca minimizar la distancia entre vectores de fragmentos de código similares y maximizarla para fragmentos disímiles. La pérdida de contraste se define como:

\[
L(y, d) = y \cdot d^2 + (1 - y) \cdot \max(0, m - d)^2,
\]

donde:
\begin{itemize}
    \item \( y \in \{0, 1\} \) es la etiqueta que indica si el par de fragmentos de código es similar (\( y = 1 \)) o disímil (\( y = 0 \)).
    \item \( d \) es la distancia euclidiana entre los vectores de los fragmentos generados por las subredes.
    \item \( m > 0 \) es un margen que define la distancia mínima esperada entre fragmentos disímiles.
\end{itemize}

El término \( y \cdot d^2 \) penaliza pares similares que están demasiado separados en el espacio vectorial, mientras que \( (1 - y) \cdot \max(0, m - d)^2 \) penaliza pares disímiles cuya distancia es menor que el margen \( m \). Esta formulación garantiza que los pares similares se agrupen en el espacio vectorial, mientras que los disímiles se alejen al menos una distancia \( m \).\\

El modelo fue optimizado mediante el optimizador Adam, conocido por su capacidad para manejar problemas de optimización no convexa en redes profundas. El aprendizaje se realizó durante 20 épocas con un tamaño de lote de 128, ajustando la tasa de aprendizaje inicial a 0.001. Durante el entrenamiento, se evaluó la pérdida en cada lote para garantizar la convergencia del modelo.\\

La arquitectura propuesta combina convoluciones unidimensionales y capas densas para extraer características relevantes de los fragmentos de código, aprovechando la estructura del AST como entrada. La función de pérdida de contraste, junto con la métrica de distancia euclidiana, permitió entrenar un modelo robusto capaz de identificar similitudes estructurales y funcionales. Este enfoque demuestra cómo las redes siamesas pueden superar las limitaciones de métodos tradicionales al enfocarse en aspectos semánticos y contextuales del código.

\section{Version2}

\subsection{Estructura y Proceso de Entrenamiento de la Red Neuronal Siamesa} 

Esta implementación de la red neuronal siamesa está diseñada para comparar fragmentos de código mediante la métrica de distancia \( L1 \), que calcula la diferencia absoluta entre las representaciones vectoriales generadas para cada fragmento. La arquitectura de la red se basa en una clase personalizada, `SiameseNeuralNetwork`, que encapsula la construcción, entrenamiento y predicción del modelo, proporcionando una solución modular y reutilizable para tareas de análisis de similitud de código.

\subsection{Arquitectura de la Red} 

La red neuronal consta de dos subredes idénticas que procesan los vectores de características extraídos de los fragmentos de código. Cada subred incluye las siguientes capas principales:

\begin{enumerate}
    \item \textbf{Preprocesamiento de la entrada:} La entrada, un vector de 65 dimensiones, se ajusta a una forma compatible con las operaciones de convolución unidimensional mediante una capa de \textit{Reshape}.
    \item \textbf{Capas de convolución unidimensional (Conv1D):} Se emplean dos capas convolucionales con tamaños de kernel de 8 y 4, y activación ReLU. Estas capas extraen patrones locales de las características del código.
    \item \textbf{Operaciones de Max Pooling:} Cada capa de convolución es seguida por una operación de \textit{max pooling} con un tamaño de ventana de 2, lo que reduce dimensionalidad mientras se preservan características relevantes.
    \item \textbf{Capas densas con regularización \( L2 \):} La salida convolucional se aplana y pasa por tres capas densas con activaciones ReLU y softmax. Estas capas incluyen regularización \( L2 \) para prevenir el sobreajuste y mejorar la generalización.
\end{enumerate}

La salida de cada subred es un vector de alta dimensión \( \mathbf{h} \in \mathbb{R}^d \) que encapsula las propiedades semánticas y estructurales del código.

\subsection{ Métrica de Distancia \( L1 \)}

La comparación entre los fragmentos de código se realiza mediante una capa de distancia \( L1 \), definida como:

\[
d_{L1}(\mathbf{h}_1, \mathbf{h}_2) = \sum_{i=1}^{d} |\mathbf{h}_{1,i} - \mathbf{h}_{2,i}|,
\]

donde \( \mathbf{h}_1 \) y \( \mathbf{h}_2 \) son los vectores generados por las subredes para cada fragmento de código. Esta distancia captura diferencias absolutas, permitiendo una medida robusta y sencilla de similitud.

\subsection{Proceso de Entrenamiento} 

El entrenamiento se realiza mediante la función de pérdida de entropía cruzada binaria, que se utiliza para clasificar pares de código como similares o disímiles. La función de pérdida se define como:

\[
L(y, \hat{y}) = -\frac{1}{N} \sum_{i=1}^{N} \left[ y_i \log(\hat{y}_i) + (1 - y_i) \log(1 - \hat{y}_i) \right],
\]

donde:
- \( y_i \) es la etiqueta verdadera (1 para pares similares, 0 para disímiles).
- \( \hat{y}_i \) es la probabilidad predicha por la red.
- \( N \) es el tamaño del lote.

Se utiliza el optimizador Adam con una tasa de aprendizaje inicial de 0.001, lo que garantiza una convergencia eficiente al encontrar mínimos locales en el espacio de pérdida.

El modelo se entrena durante 20 épocas, con un tamaño de lote de 32, y se evalúa utilizando un conjunto de validación que representa el 20\% de los datos originales. Este enfoque garantiza que la red aprenda a diferenciar fragmentos de código similares y disímiles mientras mantiene la capacidad de generalizar a nuevos datos.

\subsection{Predicción de Similitud} 

La red predice la similitud entre dos fragmentos de código generando un valor entre 0 y 1 mediante una capa densa con activación sigmoide. Un valor cercano a 1 indica alta similitud, mientras que valores cercanos a 0 reflejan una mayor diferencia. El proceso de predicción incluye los siguientes pasos:
\begin{enumerate}
    \item Los vectores de características de ambos fragmentos de código se pasan a las subredes.
    \item Se calcula la distancia \( L1 \) entre las salidas de las subredes.
    \item La capa sigmoide produce la probabilidad final de similitud.
\end{enumerate}



Esta implementación presenta un diseño modular y robusto que combina convoluciones unidimensionales, regularización \( L2 \), y la métrica de distancia \( L1 \) para abordar el problema de similitud de código. Además, el uso de la entropía cruzada binaria como función de pérdida asegura que la red optimice de manera efectiva su capacidad para distinguir entre fragmentos de código similares y disímiles. Esta arquitectura es particularmente adecuada para aplicaciones donde los datos presentan patrones estructurales complejos que requieren una evaluación precisa y escalable. 