\chapter{Preliminares}\label{chapter:state-of-the-art}
Esto  tengo que mejorarlo o cambiarlo todo
El análisis de similitud de código es un campo que ha evolucionado significativamente desde sus inicios en la década de 1970. La detección de similitudes en el código fuente es crucial en diversas aplicaciones, como la detección de plagio, la refactorización de código, la revisión de código y el desarrollo de herramientas de asistencia a la programación. Este estado del arte presenta una revisión exhaustiva de los desarrollos históricos, las metodologías y tecnologías utilizadas, así como los avances recientes en el análisis de similitud de código.

Los orígenes del análisis de similitud de código se remontan a la década de 1970, cuando las instituciones académicas comenzaron a buscar métodos para detectar plagio en tareas de programación. Este problema surgió debido al creciente número de cursos de programación y la necesidad de evaluar de manera justa las habilidades de los estudiantes. Durante este período, se desarrollaron algoritmos fundamentales de coincidencia de cadenas que sentaron las bases para comparar secuencias de texto, incluido el código.

Uno de los algoritmos más influyentes desarrollados durante este tiempo fue el algoritmo de Knuth-Morris-Pratt (KMP), presentado en 1970. El algoritmo KMP permite buscar patrones dentro de una cadena de texto de manera eficiente, evitando la necesidad de retroceder en el texto durante la búsqueda. Aunque originalmente diseñado para procesamiento de texto, su aplicabilidad para la comparación de secuencias de código fue reconocida rápidamente \cite{knuth1977fast}. Los algoritmos de coincidencia de cadenas como KMP, junto con otros como el algoritmo de Boyer-Moore \cite{boyer1977fast}, proporcionaron herramientas básicas pero efectivas para la detección de similitudes en el código.

En la década de 1990, el análisis de similitud de códigos avanzó significativamente con el desarrollo de herramientas especializadas para la detección de plagio. Una de las herramientas más influyentes fue MOSS (Measure of Software Similarity), creada por Alex Aiken en la Universidad de Stanford. MOSS se diseñó específicamente para identificar similitudes estructurales y sintácticas en programas, superando las limitaciones de las comparaciones textuales simples \cite{aiken1994moss}.

MOSS funciona mediante la normalización del código antes de la comparación, eliminando comentarios y espacios en blanco, y renombrando variables a nombres genéricos. Este enfoque permite a MOSS detectar similitudes en la estructura y la lógica subyacente del código, incluso cuando los cambios superficiales han sido realizados para ocultar el plagio. La capacidad de MOSS para comparar la estructura y sintaxis del código en lugar de solo el texto hizo que fuera una herramienta pionera en el ámbito de la similitud de código.

La adopción de MOSS y herramientas similares tuvo un impacto significativo en el ámbito académico. Estas herramientas permitieron a los educadores detectar plagio en grandes conjuntos de tareas de programación de manera eficiente, manteniendo la integridad académica. La capacidad de identificar similitudes estructurales en el código contribuyó a la equidad en la evaluación de los estudiantes y a la promoción de prácticas de programación éticas.

Los árboles de sintaxis abstracta (AST) surgieron en la década de 1980, principalmente en el contexto del diseño de compiladores. Los AST representan la estructura jerárquica de la sintaxis del código fuente, permitiendo un análisis más profundo y significativo del código más allá de la simple comparación de texto \cite{aho1986compilers}. \\

Inicialmente, los AST fueron utilizados en los compiladores para representar la estructura sintáctica del código fuente de manera que fuera fácil de analizar y manipular. Un AST descompone el código en sus componentes sintácticos fundamentales, organizados jerárquicamente. Esta representación facilita la detección de errores de sintaxis y la optimización del código durante el proceso de compilación. \\

En la década de 2000, los AST comenzaron a ser utilizados en diversas herramientas para el análisis, la transformación y la detección de similitudes de código \cite{muchnick1997advanced}. Este cambio permitió comparaciones de código más sofisticadas y significativas, ya que los AST encapsulan la estructura sintáctica de los programas. En lugar de comparar solo el texto del código, el análisis basado en AST permite detectar similitudes en la estructura lógica y sintáctica del código, proporcionando una visión más detallada y precisa. \\

ANTLR (Another Tool for Language Recognition) es herramienta que se utiliza para generar analizadores léxicos y sintácticos a partir de gramáticas definidas por el usuario. Es utilizada en la compilación y el procesamiento de lenguajes, ya que permite transformar el código fuente en estructuras de datos que pueden ser manipuladas.??????????????????????????????????????????????????????

I. Baxter, A. Yahin, L. Moura, M. Anna, y L. Bier. Presentaron métodos simples y prácticos para detectar clones exactos y casi coincidentes utilizando AST en el código fuente de programas. El método se basaba en hashing, primero se analizaba el código fuente para producir un AST, y luego se aplicaban tres algoritmos principales para encontrar clones. El primer algoritmo (algoritmo básico) detectaba clones de subárboles, el segundo (algoritmo de detección de secuencia) detectaba secuencias de tamaño variable de clones de subárboles, y el tercer algoritmo (último algoritmo) buscaba clones casi coincidentes más complejos intentando generalizar combinaciones de otros clones. Para que los algoritmos encuentren clones de subárboles funcionen, cada subárbol se comparaba con todos los demás subárboles para igualdad. El método es sencillo de implementar, pero los algoritmos realizan mejor trabajo en un gráfico de flujo de datos que en árboles, lo cual podría resultar en un alto número de falsos positivos al detectar clones casi coincidentes.\\

 B. N. Pellin presentó una técnica para detectar autoría de código fuente. Utilizó técnicas de aprendizaje automático para transformar el código fuente en árboles de sintaxis abstracta y luego dividir el árbol en funciones. El árbol de cada función se consideraba un documento, con un autor dado. Esta colección se alimentaba a un paquete SVM usando un kernel que opera en datos estructurados en árbol. El clasificador se entrenó con código fuente de dos autores y luego era capaz de predecir cuál de los dos autores escribió una nueva función. El método logró alcanzar entre el 67\% y el 88\% de precisión de clasificación en el conjunto de programas examinados. Sin embargo, el método es altamente vulnerable a la manipulación del código fuente, un traductor o ofuscador avanzado de código fuente podría destruir los patrones que su clasificador usa para identificar autores, y también requiere saber el conjunto de posibles autores.\\
 
 J. Son, S. Park, y S. Park 6 propusieron un sistema de detección de plagio que utiliza núcleos de árboles de análisis. El papel de los núcleos de árboles de análisis en el sistema es manejar la información estructural dentro de los programas fuente y medir la similitud entre los árboles de análisis extraídos. El sistema realiza 100\% de precisión para un ataque simple y no se ve afectado por ataques estructurales. El sistema es independiente de los lenguajes de programación. Debido a la estructura de los programas copiados, que incluye mucho basura abundante, muchos sistemas de detección de plagio fallan en detectar plagio. Sin embargo, el sistema está expuesto a presentar resultados falsos positivos porque los valores de similitud de los núcleos de árboles aumentan demasiado rápido para manejarlos, y el valor de los núcleos entre dos árboles diferentes es típicamente mucho menor que el valor entre los mismos árboles. \cite{smith2020enhanced}

A principios de la década de 2000, la investigación se centró en extraer características de los AST para perfeccionar el análisis de similitud de código. Se desarrollaron técnicas para comparar subárboles de AST, lo que permitió detectar similitudes en diferentes niveles de granularidad de la estructura del código. \\

Comparar subárboles de AST permitió a los investigadores y desarrolladores detectar similitudes no solo en el nivel superficial del código, sino también en patrones más profundos y complejos \cite{kam2005analyzing}. Esta técnica facilitó la identificación de fragmentos de código que compartían estructuras similares, aunque no fueran idénticos textualmente. \\

M. Duracik, E. Kirsak, y P. Hrkut. \cite{duracik2018source} Desarrollaron un sistema que se enfoca en representar el código fuente utilizando AST para detectar plagio. El sistema representa el código fuente utilizando hashing y vectores de características. Realizaron un experimento basado en estos dos enfoques y trataron de calcular la similitud de clases así como de métodos en un conjunto de datos de código fuente; el cual consiste en 59 envíos de estudiantes. Intentaron minimizar la comparación absoluta de similitud (abordando la debilidad del algoritmo MOSS) pero su sistema no pudo lograr escalabilidad y también se generaron algunas coincidencias falsas positivas a valores más bajos. Sin embargo, el sistema fue capaz de probar que generar vectores utilizando AST es la forma más adecuada de representar el código fuente.

Word2Vec es una técnica de aprendizaje profundo que transforma palabras en vectores de números de alta dimensión, conocidos como embeddings. Los modelos Word2Vec se entrenan utilizando grandes corpus de texto y capturan relaciones semánticas entre las palabras.??????????????????????????????????????  \\


La integración del aprendizaje automático en el análisis de similitud de códigos comenzó en la década de 2010. Los algoritmos de aprendizaje automático mejoraron la precisión de la detección de similitudes mediante el aprendizaje de patrones a partir de grandes conjuntos de datos de código. \\ 

El uso de redes neuronales comenzó a expandirse en este período. Las redes neuronales recurrentes (RNN) y las redes neuronales convolucionales (CNN) fueron entrenadas para procesar y comparar fragmentos de código, generando representaciones vectoriales que capturaban relaciones complejas y patrones estructurales. Estas técnicas permitieron una comparación más precisa y eficiente de fragmentos de código, superando las limitaciones de los métodos tradicionales. \\

Alrededor de 2014, los marcos de aprendizaje profundo comenzaron a aplicarse al análisis de código. Las redes neuronales profundas (DNN) se entrenaron para generar incrustaciones vectoriales de fragmentos de código, permitiendo una comparación más precisa y eficiente de similitudes. Estas representaciones vectoriales capturaban relaciones complejas y patrones estructurales, mejorando significativamente la precisión de la detección de similitudes. \\

Un enfoque destacado en este contexto son las redes neuronales siamesas, que se utilizan para aprender representaciones semánticas similares entre pares de fragmentos de código \cite{smith2020enhanced}. Estas redes utilizan dos ramas idénticas con pesos compartidos para procesar dos entradas de código diferentes simultáneamente. Al final de cada rama, se obtienen representaciones vectoriales que luego se comparan directamente o a través de medidas de distancia como la distancia euclidiana o la similitud del coseno. Este enfoque permite capturar relaciones más sutiles y contextuales entre fragmentos de código, superando las limitaciones de los métodos tradicionales basados en características simples.

Aqui hablar sobre las redes neuronales siamesas

En la década de 2020, los modelos avanzados de aprendizaje profundo, en particular los transformadores, se han aplicado cada vez más al análisis de código. Estos modelos han demostrado ser altamente efectivos para capturar las complejidades del código y mejorar las capacidades de detección de similitudes. \\

Ejemplos notables incluyen Codex de OpenAI y GitHub Copilot, que utilizan grandes cantidades de datos de código para tareas como completar código y detectar similitudes. Los transformadores son capaces de manejar secuencias largas y capturar dependencias a largo plazo en el código, lo que los hace especialmente adecuados para el análisis de similitud de código \cite{vaswani2017attention}. \\

Las redes neuronales gráficas (GNN) también han ganado prominencia en el análisis de similitud de código. Las GNN tratan los AST como gráfos, capturando estructuras de código intrincadas para un análisis de similitud más matizado. Este enfoque permite una representación más rica y detallada del código, mejorando la precisión de las comparaciones.

